# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yeFUGXiU-uY3ClKJbLK4sOTx_I-wxJIL

## **Project: Generating Natural Human Messaging Data (with Microsoft Phi-2 - English)**
Objective: To generate human-like natural messaging data using Large Language Models (LLMs) and report on the observed performance, specifically focusing on the capabilities of the smaller and more efficient Microsoft Phi-2 model in an English context.

**Day 1:** *Project Setup and Model Selection*
Today, we'll set up the foundation of our project: install necessary libraries, load the Microsoft Phi-2 model, design our data structure, and define our initial scenarios.

Important Note: Make sure your Colab session is set to GPU runtime.

In Colab, click Runtime -> Change runtime type in the top menu.
Ensure Hardware accelerator is set to T4 GPU or A100 GPU (for Colab Pro).
"""

# Day 1: Project Setup and Model Selection (Microsoft Phi-2 - English)

# 1. Install necessary libraries
# Hugging Face Transformers library and its dependencies
!pip install transformers accelerate bitsandbytes -q
!pip install --upgrade transformers -q # Upgrade for compatibility

import os
import json
import datetime
import torch # For GPU check and model loading

# Import necessary classes from Hugging Face library
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, set_seed

print("Libraries successfully installed and imported.")

# 2. Load the LLM Model and Tokenizer
# Choosing Microsoft Phi-2 model.
# This model performs remarkably well despite its small size.
MODEL_NAME = "microsoft/phi-2"

print(f"\nModel to be loaded: {MODEL_NAME}")

# Initialize generator globally for access in subsequent days.
generator = None

try:
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    # Phi-2 doesn't have a default pad_token_id, so we assign eos_token_id.
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # Load model in 8-bit for memory optimization and onto GPU.
    # device_map="auto" ensures automatic placement on GPU.
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        load_in_8bit=True,      # Memory optimization (Quantization)
        torch_dtype=torch.float16, # High-performance computation type
        device_map="auto",      # Automatic placement on GPU
        trust_remote_code=True  # Required for some custom model architectures
    )

    # Set random seed for reproducibility of generations.
    set_seed(42)

    # Create a text generation pipeline
    # IMPORTANT: 'device' argument is omitted because device_map="auto" is used during model loading.
    generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer
    )
    print(f"Model '{MODEL_NAME}' and tokenizer successfully loaded.")

except Exception as e:
    print(f"Error: An issue occurred while loading the model: {e}")
    print("Ensure your Colab runtime type is 'GPU' and has sufficient RAM.")
    print("You might consider trying a smaller model or checking the 'trust_remote_code=True' parameter.")
    generator = None # Set generator to None in case of an error

# 3. Data Structure Design
# A JSON-like structure to store generated messages in an organized way.
# The 'messages' list will contain each message in the dialogue (role, text).

all_generated_message_data = [] # Empty list to hold all generated data
print("\nData structure design completed (all_generated_message_data list created).")

# 4. Create Initial Example Scenarios
# Define different scenarios for which we'll generate messages using the LLM.
# 'prompt_starter': The opening line of the dialogue or an email subject.
# 'context': Information guiding the model on the tone and style of message generation.
scenarios = [
    {
        "id": "friend_chat_01",
        "description": "A casual chat between two close friends about weekend plans.",
        "prompt_starter": "Alice: Hey Bob, do you have any plans for the weekend? Maybe we could do something together.",
        "context": "Two close friends talking casually in English. Responses should be short and fluid. Use the speakers' names in the response."
    },
    {
        "id": "work_message_01",
        "description": "A brief informational message from a manager to their team about a project deadline.",
        "prompt_starter": "Subject: Project ALPHA Deadline Reminder\nHi Team,\nJust a quick reminder that Project ALPHA's deadline is this Friday.",
        "context": "Formal and professional work environment. Messages should be clear, concise, and informative. Use 'Team Member:' as a prefix for replies."
    },
    {
        "id": "customer_service_01",
        "description": "A customer service representative's short reply to a customer's inquiry about their shipping status.",
        "prompt_starter": "Customer: Hi, I'd like to know if my order has been shipped. My order number is #123456.",
        "context": "A short, informative, and polite dialogue between customer service and a customer. Replies should be direct and to the point. Use 'Customer Service:' as a prefix for replies."
    },
    {
        "id": "family_dialogue_01",
        "description": "A conversation between a mother and son about dinner plans.",
        "prompt_starter": "Mom: Honey, I'm thinking about what to make for dinner. What would you like to eat?",
        "context": "A warm and casual conversation between a mother and son. Options can be suggested. Use the speakers' names in the response."
    },
    {
        "id": "e_learning_question_01",
        "description": "A student asking their instructor a brief question about class notes on an online learning platform.",
        "prompt_starter": "Student: Hello Professor, where can we access the notes for week 5?",
        "context": "A formal but helpful dialogue between a student and an instructor. Replies should be clear and guiding. Use 'Professor:' as a prefix for replies."
    }
]

print("\nInitial scenarios successfully defined.")
print("-" * 50)

"""**Day 2:** Data Generation and Initial Trials
Today, we'll generate messages from the LLM based on our defined scenarios and conduct initial quality assessments. In this phase, we run simple trials to observe how the model responds.
"""

# Day 2: Data Generation and Initial Trials (Microsoft Phi-2 - English)

# Function to generate messages from LLM (using Hugging Face pipeline)
# Adjusted for Phi-2's likely best prompt format.
def generate_chat_completion_hf(generator, messages_history, temperature=0.7, max_new_tokens=150):
    if generator is None:
        return "Model not loaded, cannot generate response."

    # For Phi-2, a simple question-answer format or direct dialogue history often works best.
    # Character prefixes (e.g., "Alice:", "Bob:") help the model understand who is speaking.
    full_prompt_text = ""
    for msg in messages_history:
        if msg["role"] == "system":
            # Add the system prompt at the very beginning to define overall behavior.
            full_prompt_text += f"Instruction: {msg['content']}\n"
        elif msg["role"] == "user":
            # User messages
            full_prompt_text += f"{msg['content']}\n"
        elif msg["role"] == "assistant":
            # Assistant messages
            full_prompt_text += f"{msg['content']}\n"

    # Add a prefix at the end where we expect the model to respond.
    # This guides the model on which character should speak next.
    last_role = messages_history[-1]['role'] if messages_history else ""
    if last_role == "user":
        system_context = messages_history[0]['content'].lower() if messages_history and messages_history[0]['role'] == 'system' else ""
        if "friend chat" in system_context:
            # If Alice initiated, Bob should respond. If Bob responded, Alice should ask next.
            if "alice:" in messages_history[-1]['content'].lower():
                full_prompt_text += "Bob:"
            elif "bob:" in messages_history[-1]['content'].lower():
                full_prompt_text += "Alice:"
            else: # Default for first turn
                full_prompt_text += "Bob:"
        elif "work environment" in system_context:
            full_prompt_text += "Team Member:"
        elif "customer service" in system_context:
            full_prompt_text += "Customer Service:"
        elif "family dialogue" in system_context:
            # If Mom initiated, Son should respond. If Son responded, Mom should ask next.
            if "mom:" in messages_history[-1]['content'].lower():
                full_prompt_text += "Son:"
            elif "son:" in messages_history[-1]['content'].lower():
                full_prompt_text += "Mom:"
            else: # Default for first turn
                full_prompt_text += "Son:"
        elif "e-learning" in system_context:
            full_prompt_text += "Professor:"
        else:
            full_prompt_text += "Response:" # Default

    prompt_for_model = full_prompt_text.strip() + "\n" # Newline for the model's response.

    try:
        outputs = generator(
            prompt_for_model,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True, # For randomness
            pad_token_id=generator.tokenizer.eos_token_id, # Padding token
            num_return_sequences=1,
            return_full_text=False # Get only the generated text
        )
        generated_text = outputs[0]['generated_text'].strip()

        # Post-processing: Clean up redundant prefixes or repeated prompt text
        clean_text = generated_text
        for line in full_prompt_text.split('\n'):
            if line.strip() and clean_text.lower().startswith(line.strip().lower()):
                clean_text = clean_text[len(line.strip()):].strip()

        # Specific cleanup for Phi-2's common issues like repeating names or instructions
        clean_text = clean_text.replace("Alice:", "").replace("Bob:", "").replace("Team Member:", "")
        clean_text = clean_text.replace("Customer Service:", "").replace("Mom:", "").replace("Son:", "")
        clean_text = clean_text.replace("Professor:", "").replace("Response:", "")
        clean_text = clean_text.replace("Instruction:", "").replace("Output:", "").replace("Instruct:", "")

        # Remove any leading/trailing spaces or empty lines
        clean_text = clean_text.strip()
        if clean_text.startswith("\n"):
            clean_text = clean_text.lstrip("\n").strip()

        return clean_text

    except Exception as e:
        print(f"Error during message generation (Hugging Face): {e}")
        return ""

print("Message generation function (generate_chat_completion_hf) defined.")

# Generate message series for each scenario and conduct initial trials
generated_trial_data = [] # Separate list for initial trials

if generator is None:
    print("Model not loaded, Day 2 trials cannot proceed. Please check Day 1.")
else:
    for scenario in scenarios:
        print(f"\n--- Scenario ID: {scenario['id']} ---")
        print(f"Description: {scenario['description']}")

        # Define system role and context
        messages = [
            {"role": "system", "content": f"You are an assistant that generates natural, human-like messages. {scenario['context']}"}
        ]

        # Add the initial user message (from prompt_starter)
        initial_user_message = scenario['prompt_starter'].strip()
        messages.append({"role": "user", "content": initial_user_message})

        print(f"\nInitial Messages:\n{json.dumps([m for m in messages if m['role'] != 'system'], indent=2, ensure_ascii=False)}")

        # Get the first response from the model
        # Experiment with different temperature and max_new_tokens values
        generated_response_1 = generate_chat_completion_hf(generator, messages, temperature=0.7, max_new_tokens=100)
        messages.append({"role": "assistant", "content": generated_response_1}) # Add response to history

        print(f"\nFirst Response from Model (Temp: 0.7, Max Tokens: 100):\n{generated_response_1}")

        # Add a second turn (user and model response)
        # Add a simple follow-up user message to continue the dialogue
        follow_up_prompt = ""
        if "friend" in scenario['description'].lower():
            if "Alice:" in initial_user_message:
                follow_up_prompt = "Bob: Sounds great! What do you have in mind?"
            else:
                follow_up_prompt = "Alice: Sounds great! What do you have in mind?"
        elif "work" in scenario['description'].lower():
            follow_up_prompt = "Team Member: Thanks for the reminder. What are our next steps on this?"
        elif "customer" in scenario['description'].lower() or "instructor" in scenario['description'].lower():
            follow_up_prompt = "Customer: Understood, thank you. Can I reach out to you again if I have more questions?"
        elif "family" in scenario['description'].lower():
            if "Mom:" in initial_user_message:
                follow_up_prompt = "Son: That sounds good, Mom. Do you have any other ideas?"
            else:
                follow_up_prompt = "Mom: That sounds good, honey. Do you have any other ideas?"
        else:
            follow_up_prompt = "User: Thanks. Can I ask you about something else?"

        messages.append({"role": "user", "content": follow_up_prompt})
        generated_response_2 = generate_chat_completion_hf(generator, messages, temperature=0.8, max_new_tokens=120)
        messages.append({"role": "assistant", "content": generated_response_2})

        print(f"\nSecond Response from Model (Temp: 0.8, Max Tokens: 120):\n{generated_response_2}")

        # Save the generated data (for this trial)
        entry = {
            "scenario_id": scenario["id"],
            "scenario_description": scenario["description"],
            "generated_at": datetime.datetime.now().isoformat(),
            "messages": [msg for msg in messages if msg["role"] != "system"] # Exclude system messages
        }
        generated_trial_data.append(entry)

        # Quality Assessment (Manual Review)
        print("\n--- Quality Assessment (Manual) ---")
        print("Read the generated messages above and note their naturalness, fluency, and relevance to the scenario.")
        print("Are there any repetitive or nonsensical phrases? Is the conversation context well-followed?")
        print("-" * 50)

    # Save all trial data to a file
    with open("day_2_initial_trials_phi2_en.json", "w", encoding="utf-8") as f:
        json.dump(generated_trial_data, f, ensure_ascii=False, indent=4)
    print("\nDay 2 trials saved to 'day_2_initial_trials_phi2_en.json'.")

"""**Day 3:** Refinement and Diversification
Today, based on feedback from yesterday's trials, we'll optimize model parameters and generate longer, context-aware dialogues. Finally, we'll clean and save all generated data in a standardized format.
"""

# Day 3: Refinement and Diversification (Microsoft Phi-2 - English)

# The generate_chat_completion_hf function is already defined and improved in Day 2.

print("Message generation function (generate_chat_completion_hf) is ready for use.")

# 1. Model Parameter Tuning and Regeneration
# Based on insights from yesterday's trials, let's optimize model parameters.
# For Phi-2, slightly lower temperature might yield more coherent results.
optimized_temperature = 0.65 # Slightly lower temperature for more coherence
optimized_max_new_tokens = 150 # Max new tokens for longer responses

print(f"\nParameters optimized: Temperature = {optimized_temperature}, Max New Tokens = {optimized_max_new_tokens}")
print("\n--- Data Generation with New Parameters (Longer and Contextual Dialogues - Microsoft Phi-2 - English) ---")

final_generated_data = [] # Our final dataset

if generator is None:
    print("Model not loaded, Day 3 data generation cannot proceed. Please check Day 1.")
else:
    # Loop through each scenario
    for scenario in scenarios:
        print(f"\n--- Scenario ID: {scenario['id']} (Final Generation) ---")
        print(f"Description: {scenario['description']}")

        # Start a new dialogue history for each scenario
        current_dialog_messages = [
            {"role": "system", "content": f"You are an assistant that generates natural, human-like messages. {scenario['context']}"}
        ]

        # List to hold all generated messages (user and assistant) for this scenario
        generated_messages_for_scenario = []

        # Add the initial user message
        initial_user_message = scenario['prompt_starter'].strip()
        current_dialog_messages.append({"role": "user", "content": initial_user_message})
        generated_messages_for_scenario.append({"role": "user", "content": initial_user_message})

        print(f"  Starting: {initial_user_message[:100]}...")

        # Dialogue Flow and Context Tracking: Generate dialogues with multiple messages
        num_dialog_turns = 4 # E.g., 4 turns of dialogue (4 assistant responses and 4 user follow-up messages)

        for i in range(num_dialog_turns):
            # 1. Get the assistant's response from the model
            assistant_response = generate_chat_completion_hf(
                generator,
                current_dialog_messages,
                temperature=optimized_temperature,
                max_new_tokens=optimized_max_new_tokens
            )

            if not assistant_response or "Model not loaded" in assistant_response:
                print(f"  Error: Could not get response from model or model not loaded, stopping generation for this scenario.")
                break # Stop this scenario in case of error

            current_dialog_messages.append({"role": "assistant", "content": assistant_response})
            generated_messages_for_scenario.append({"role": "assistant", "content": assistant_response})
            print(f"  Turn {i+1} - Assistant: {assistant_response[:100]}...") # Print first 100 characters

            # 2. Add a simple follow-up user message to continue the dialogue
            next_user_prompt = ""
            if "friend" in scenario['description'].lower():
                # Alternate speaker names for friend chat
                if "alice:" in generated_messages_for_scenario[-1]['content'].lower() or \
                   ("alice:" in initial_user_message.lower() and i == 0):
                    next_user_prompt = "Bob: Sounds good! What else were you thinking?"
                else:
                    next_user_prompt = "Alice: Sounds good! What else were you thinking?"
            elif "work" in scenario['description'].lower():
                next_user_prompt = "Team Member: What are our next steps for this task?"
            elif "customer" in scenario['description'].lower() or "instructor" in scenario['description'].lower():
                next_user_prompt = "Customer: Okay, thanks. Could you also tell me about X?"
            elif "family" in scenario['description'].lower():
                # Alternate speaker names for family dialogue
                if "mom:" in generated_messages_for_scenario[-1]['content'].lower() or \
                   ("mom:" in initial_user_message.lower() and i == 0):
                    next_user_prompt = "Son: Okay Mom, that sounds good. Any other ideas?"
                else:
                    next_user_prompt = "Mom: Okay honey, that sounds good. Any other ideas?"
            else:
                next_user_prompt = "User: Can we continue this conversation?"

            current_dialog_messages.append({"role": "user", "content": next_user_prompt})
            generated_messages_for_scenario.append({"role": "user", "content": next_user_prompt})
            print(f"  Turn {i+1} - User: {next_user_prompt[:100]}...")


        # Data Preprocessing: Clean, format, and save generated texts
        # Save final dialogues for each scenario as a separate entry.
        scenario_data_entry = {
            "scenario_id": scenario["id"],
            "scenario_description": scenario["description"],
            "generated_at": datetime.datetime.now().isoformat(),
            "messages": generated_messages_for_scenario # Only user and assistant messages
        }
        final_generated_data.append(scenario_data_entry)

    print("\nData generation completed for all scenarios.")

    # Save all final data in JSON format
    output_filename_final = "natural_messaging_data_phi2_en.json"
    with open(output_filename_final, "w", encoding="utf-8") as f:
        json.dump(final_generated_data, f, ensure_ascii=False, indent=4)

    print(f"\nFinal dataset successfully saved to '{output_filename_final}'.")
print("-" * 50)

"""**Day 4:** Reporting and Sharing
On the final day, we'll analyze the quality of the generated data, prepare a report, and make it shareable. This part is done using Markdown cells and comments in Colab. You can write your report in a new "Text" cell in Colab using the Markdown template below.
"""

# Day 4: Reporting and Sharing (Microsoft Phi-2 - English)

# 1. Analysis of Results (Manual and Simple Metrics)
# At this stage, we should review the 'natural_messaging_data_phi2_en.json' file we just created.
# We can perform simple analyses with Python:

print("--- Day 4: Reporting and Sharing ---")
print("\nLoading final dataset from 'natural_messaging_data_phi2_en.json'...")

output_filename_final = "natural_messaging_data_phi2_en.json"

try:
    with open(output_filename_final, "r", encoding="utf-8") as f:
        loaded_data = json.load(f)
    print(f"Total {len(loaded_data)} scenario entries loaded.")

    total_messages = 0
    total_words = 0
    for entry in loaded_data:
        for msg in entry['messages']:
            total_messages += 1
            total_words += len(msg['content'].split())

    print(f"Total messages generated: {total_messages}")
    print(f"Total words generated: {total_words}")
    print(f"Average message length: {total_words / total_messages:.2f} words")

except FileNotFoundError:
    print(f"Error: '{output_filename_final}' file not found. Check previous steps.")
    loaded_data = [] # Empty list in case of error

print("\n--- Key Notes for Quality Analysis and Report ---")
print("1. **Naturalness and Fluency:** Read the dialogues in each scenario and evaluate how human-like the messages are.")
print("2. **Context Tracking:** Check how well the dialogues follow each other and if previous messages were understood.")
print("3. **Variety:** Observe if responses are monotonous and if different tones and styles are used across scenarios.")
print("4. **Nonsense/Repetition:** Check for any nonsensical or repetitive phrases. (This might relate to the 'temperature' setting.)")
print("5. **Scenario Appropriateness:** Evaluate if each scenario (friend chat, work message, customer service, etc.) generated messages appropriate to the defined context.")
print("6. **Model Differences:** Compare the performance and characteristics of Phi-2 with larger models (e.g., Mistral or OpenAI).")

print("\n--- Report Writing (Should be done in a Markdown Cell below) ---")
print("Write a report outlining the project's objective, methods used, results obtained (with good and bad examples), challenges faced, and future steps.")
print("You can add a new 'Text' cell in Colab and write your report in Markdown format.")

print("\n--- Example Report Structure (Specific to Microsoft Phi-2 - English) ---")

"""# Project Report: Generating Natural Human Messaging Data (with Microsoft Phi-2 - English)

## 1. Project Objective
The primary objective of this project was to generate human-like natural messaging data using a **small-scale open-source Large Language Model (LLM)**, specifically **Microsoft Phi-2**, and evaluate its capabilities across different English chat scenarios. The aim was to assess the suitability of Phi-2 for synthetic data generation in resource-constrained environments (like Google Colab's free tier).

---

## 2. Methodology
The **`microsoft/phi-2`** model was utilized through the **Hugging Face Transformers library**. The model was loaded on Google Colab's GPU resources with **8-bit quantization** for memory optimization. A random seed (`set_seed(42)`) was set to ensure reproducibility of generations. For data generation, the **`temperature`** parameter was set to [e.g., 0.65] and **`max_new_tokens`** to [e.g., 150].

Various chat scenarios (friend chat, work message, customer service, family dialogue, e-learning questions) were defined, each with specific `prompt_starter` and `context` information. To ensure coherent **multi-turn dialogues**, the dialogue history was passed to the model in a **plain text prompt format** (e.g., "Speaker1: [message]\nSpeaker2: [message]") to help the model maintain context and understand who was speaking. An average of 4 turns of dialogue were generated per scenario.

---

## 3. Results Obtained
The messaging data generated with **Microsoft Phi-2** generally demonstrated **surprising naturalness and fluency** given the model's small size. It performed particularly well in straightforward question-answer formats and simple dialogue flows. The model showed satisfactory capability in following context and adhering to character prefixes in the responses.

### Example Generated Messages:

**Scenario: Friend Chat (ID: friend_chat_01)**
* **Prompt:** "Alice: Hey Bob, do you have any plans for the weekend? Maybe we could do something together."
* **Model Response:** "Bob: Hey Alice! Not really, just planning to relax. What were you thinking?"
* **Evaluation:** [Write your own evaluation here. Example: "A very natural and friendly response. The model correctly identified Bob as the next speaker and maintained a casual tone. Good fluidity."]

**Scenario: Work Message (ID: work_message_01)**
* **Prompt:** "Subject: Project ALPHA Deadline Reminder\nHi Team,\nJust a quick reminder that Project ALPHA's deadline is this Friday."
* **Model Response:** "Team Member: Thanks for the reminder! We're on track to submit by Friday. Will send an update later today."
* **Evaluation:** [Write your own evaluation here. Example: "Professional and concise. The model provided a relevant and appropriate response for a work context. The tone is suitable."]

---

## 4. Challenges Faced and Suggested Solutions
* **Prompt Format Sensitivity:** Smaller models like Phi-2 appear to be more sensitive to the exact prompt format (especially for maintaining dialogue flow) compared to larger models or OpenAI APIs. More precise guidance in the prompt might be needed for consistent quality.
* **Limited Creativity and Scope:** Due to its smaller size, Phi-2 might exhibit less creative responses or a shallower understanding of complex contexts compared to larger models. Its performance could diminish in very intricate or abstract scenarios.
* **Response Cleanup:** The model sometimes repeated parts of the prompt or generated extraneous tags. Post-processing steps (like the ones implemented in the code) are crucial to clean up these artifacts.

---

## 5. Future Steps
* Increase the size of the generated dataset and incorporate a wider variety of scenarios.
* Explore more advanced **prompt engineering techniques** and specific "chat templates" to further enhance Phi-2's dialogue capabilities.
* Experiment with different open-source LLMs (e.g., optimized versions of Mistral, or quantized versions of Llama 3) to compare performance and identify the most suitable model for specific tasks.
* Initiate a human annotation process to objectively evaluate the quality of the generated data.

---

## 6. Data Set and Report Sharing
The generated **`natural_messaging_data_phi2_en.json`** file and this report are ready for evaluation as project outputs. The Colab notebook contains all the code and steps.


"""